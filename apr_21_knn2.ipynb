{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44966e24-6134-438d-ab39-332692f7bd8f",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e6289-e33a-4aad-b910-111244e36cc3",
   "metadata": {},
   "source": [
    "Euclidean Distance: Euclidean distance measures the straight-line (shortest) distance between two points in a geometric space. It is calculated as the square root of the sum of squared differences between corresponding coordinates of the points. Euclidean distance considers both magnitude and direction.\n",
    "\n",
    "Manhattan Distance: Manhattan distance (also known as L1 distance or taxicab distance) measures the distance between two points as the sum of the absolute differences between their coordinates. It ignores diagonal shortcuts and only considers movements along gridlines (like navigating through city blocks).\n",
    "\n",
    "The main difference is in the way these metrics calculate distance. Euclidean distance accounts for diagonal shortcuts and considers both vertical and horizontal movements, whereas Manhattan distance only considers horizontal and vertical movements.\n",
    "\n",
    "The choice of distance metric can significantly affect KNN performance:\n",
    "\n",
    "Euclidean distance is sensitive to the magnitude and direction of differences. It's suitable when the underlying data relationships are isotropic (equal in all directions).\n",
    "Manhattan distance is less sensitive to direction and can be better suited when data relationships are not isotropic or when dimensions are not directly comparable (e.g., measuring distances in different units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61116a7-87de-4f71-980f-259a1f0de6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2a460b-7a86-4a4f-9776-c27923d8fc6c",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea3d38-a045-4487-9389-18efb7ecc2df",
   "metadata": {},
   "source": [
    "Selecting the optimal value of k in KNN is critical and can be done through techniques like:\n",
    "\n",
    "- Cross-Validation: Perform k-fold cross-validation on the training data for various k values and choose the one with the best performance metric (e.g., accuracy, MSE).\n",
    "- Grid Search: Combine cross-validation with grid search to systematically explore multiple hyperparameter combinations, including different k values.\n",
    "- Domain Knowledge: Consider domain-specific knowledge to make an informed choice for k.\n",
    "- Rule of Thumb: Use a heuristic like the square root of the number of data points as a starting point (but validate it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b2ac1-edff-4720-a2ef-3e33d4129230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8944d083-4ae7-463b-9d26-4dc065f440e1",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffef5d8-1b2b-44e9-9193-46f5ebc89c01",
   "metadata": {},
   "source": [
    "Choice of Distance Metric: The choice of distance metric affects how KNN calculates similarity between data points. Euclidean distance tends to work well when the relationships between features are isotropic (similar in all directions), while Manhattan distance is less sensitive to the direction of relationships.\n",
    "\n",
    "Situations for Euclidean Distance: Euclidean distance is often suitable when features are directly comparable, and the problem involves relationships that consider both magnitude and direction (e.g., geometric data).\n",
    "\n",
    "Situations for Manhattan Distance: Manhattan distance can be preferable when features are not directly comparable or when you want to downplay the effect of magnitude differences (e.g., taxicab distances in a city).\n",
    "\n",
    "The choice of distance metric should align with the characteristics of the data and the problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989b901-5fb6-4439-a338-ea12d7bfb349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a7c34c-5903-4075-a38f-febec385e9ae",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd438be0-a621-471c-bb7f-61fd2d3b52a6",
   "metadata": {},
   "source": [
    "Common hyperparameters in KNN include:\n",
    "\n",
    "- k: The number of nearest neighbors to consider.\n",
    "- Distance Metric: The metric used to calculate distances (e.g., Euclidean, Manhattan).\n",
    "- Weighting: Whether to weight the contributions of neighbors by distance.\n",
    "- Algorithm: The algorithm used to compute neighbors (e.g., ball tree, KD tree).\n",
    "\n",
    "Tuning these hyperparameters involves methods like:\n",
    "\n",
    "- Using cross-validation and grid search to evaluate performance for different hyperparameter combinations.\n",
    "- Examining performance metrics (e.g., accuracy, MSE) to choose the best hyperparameter values.\n",
    "- Balancing trade-offs (e.g., larger k for smoother predictions, smaller k for sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb917e-94d3-4224-8bb2-392898baa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74489ce-b9a2-4f37-ac85-9a151058affe",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f3f23-5552-4da0-b7a3-28f58d1020d7",
   "metadata": {},
   "source": [
    "Training Set Size: A larger training set often leads to more robust and accurate KNN models. Smaller training sets can result in overfitting.\n",
    "\n",
    "Optimizing Training Set Size: Techniques to optimize the training set size include collecting more data when possible, using resampling methods (e.g., bootstrapping), and employing techniques like k-fold cross-validation to make efficient use of available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f3ff9-79c7-4c3d-8dce-5107d9341c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aad1409-9d06-407f-9ad0-a573b4fd1ee5",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e73b-d54c-42ca-85e7-18c33a6007bb",
   "metadata": {},
   "source": [
    "Drawbacks of KNN:\n",
    "\n",
    "- Computationally Intensive: KNN can be slow for large datasets since it requires calculating distances for every data point.\n",
    "- Sensitive to Outliers: Outliers can have a strong impact on KNN predictions.\n",
    "- Curse of Dimensionality: Performance can deteriorate in high-dimensional spaces.\n",
    "- Imbalanced Data: It may perform poorly on imbalanced datasets.\n",
    "\n",
    "To overcome these drawbacks:\n",
    "\n",
    "- Optimize Algorithms: Use optimized algorithms (e.g., ball tree, KD tree) to speed up computation.\n",
    "- Outlier Handling: Identify and handle outliers appropriately.\n",
    "- Dimensionality Reduction: Apply dimensionality reduction techniques (e.g., PCA) for high-dimensional data.\n",
    "- Data Preprocessing: Address class imbalance through resampling techniques.\n",
    "- Feature Selection: Use feature selection to reduce dimensionality and focus on relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f48acf-86b1-4b2c-8e9b-9f68a249928d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
