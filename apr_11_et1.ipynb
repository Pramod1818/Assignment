{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b40854a-8296-4b3a-9d72-0df10a86951c",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fc0ff-e7c6-4db6-a6aa-c85f6e279680",
   "metadata": {},
   "source": [
    "Ensemble techniques in machine learning are methods that combine the predictions of multiple base models (individual machine learning models) to produce a more robust and accurate prediction. The idea behind ensemble techniques is to leverage the diversity of different models to reduce the risk of overfitting and improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42449cbf-9aee-4a10-9178-1b87bdf454f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c0cca46-c6ec-4673-8715-92ff077ad61f",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778308f5-adb7-427a-bd77-33af5f0ed0a6",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "- Increased Accuracy: Ensembles often outperform individual models by reducing bias and variance, leading to more accurate predictions.\n",
    "- Robustness: Combining diverse models helps make predictions more robust, reducing the impact of errors from individual models.\n",
    "- Overfitting Reduction: Ensembles can mitigate overfitting because they generalize better, especially when using bagging and boosting methods.\n",
    "- Model Selection: Ensemble methods allow you to combine the strengths of different algorithms, helping to select the best model for a particular problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f824a42-8444-418a-97c0-572e42a333b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f74dd3-6f7a-4280-bab2-0c4d860d0a69",
   "metadata": {},
   "source": [
    "Q3. What is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62ea4a-0d63-45bd-b679-881ade7f1acb",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique that aims to improve the accuracy and robustness of machine learning models. It works by training multiple base models (typically the same type of model) on different random subsets of the training data. These subsets are created using a process called bootstrapping, where data points are randomly sampled with replacement. Once the base models are trained, their predictions are aggregated, often by averaging (for regression) or voting (for classification), to make the final prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bc158-c7e2-43ae-9fdd-fad18d42fd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76639e1a-95e3-45e6-842a-bed1e8cfa0b0",
   "metadata": {},
   "source": [
    "Q4. What is boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d444f3bd-998b-4be0-baaf-9a4f8e12a29b",
   "metadata": {},
   "source": [
    "Boosting is another ensemble technique that combines multiple weak learners (usually simple models) to create a strong learner. Unlike bagging, boosting assigns different weights to data points and focuses on learning from the mistakes of previous models. It iteratively trains models, giving higher weight to data points that were misclassified by previous models and lower weight to correctly classified points. The final prediction is a weighted combination of the individual models' predictions, with more accurate models receiving higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a8a6b-58fb-481c-910a-04e7db60915b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88c1b618-2b5b-4224-bef3-b48be4c4a1ef",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82381f90-8464-4819-8454-6c5f70fc9107",
   "metadata": {},
   "source": [
    "The benefits of using ensemble techniques include:\n",
    "\n",
    "- Improved Predictive Accuracy: Ensembles often yield more accurate predictions than individual models.\n",
    "- Robustness: Ensembles are less prone to overfitting and are more resistant to noise in the data.\n",
    "- Versatility: They can be applied to various types of machine learning algorithms and tasks.\n",
    "- Model Selection: Ensembles help select the best model or combination of models for a given problem.\n",
    "- Interpretability: Ensembles can provide insights into feature importance and model performance.\n",
    "- Reduced Variance: They tend to reduce the variance of the model, making it more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969452f-d31b-4b98-804d-557320174c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "871156f8-6298-494e-a277-a2b29d6371a9",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b44e-d4ca-4be4-8573-2b1ed03effd1",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. Whether an ensemble outperforms individual models depends on several factors, including the choice of base models, data quality, problem complexity, and ensemble method. While ensembles can improve accuracy and robustness, there are situations where they may not provide substantial benefits or could be computationally expensive. It's essential to consider the specific problem and dataset when deciding whether to use an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb71fd-c225-40e4-a655-09a03e845805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310d9455-0afb-42b1-9024-2c8b3985454f",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea34e5-73d9-4271-9948-b400146ac894",
   "metadata": {},
   "source": [
    "The confidence interval (CI) using bootstrap is calculated by resampling the dataset multiple times (with replacement) to create multiple \"bootstrap samples.\" For each bootstrap sample, you compute the statistic of interest (e.g., mean, median, variance, etc.). After obtaining a distribution of these statistics from the bootstrap samples,we can construct a confidence interval.\n",
    "\n",
    "Here are the steps to calculate a bootstrap confidence interval:\n",
    "\n",
    "- Collect a dataset with N observations.\n",
    "- Randomly select N data points from the dataset with replacement to create a bootstrap sample. Repeat this process B times to generate B bootstrap samples.\n",
    "- Compute the statistic of interest (e.g., mean, median, variance) for each bootstrap sample.\n",
    "- Calculate the lower and upper percentiles of the statistic from the B bootstrap statistics to define the confidence interval.\n",
    "\n",
    "The most common confidence intervals are the 95% and 99% confidence intervals, which include the middle 95% or 99% of the bootstrap statistics, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeea90f-2f93-4937-a0c4-8b5870efd6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b664ef29-fdd0-47b6-8f7a-ff2c0a2fe048",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcef9d-aad5-4729-a54b-90121e19e753",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data with replacement. Here are the steps involved in bootstrap:\n",
    "\n",
    "- Data Collection: Start with your original dataset, which has N observations.\n",
    "\n",
    "- Resampling: Randomly select N data points from the dataset with replacement to create a bootstrap sample. This process is repeated multiple times (typically B times) to generate B bootstrap samples. Each bootstrap sample has N data points.\n",
    "\n",
    "- Statistic Calculation: For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, variance, etc.).\n",
    "\n",
    "- Bootstrap Distribution: Collect the calculated statistics from all the bootstrap samples, resulting in a distribution of the statistic.\n",
    "\n",
    "- Confidence Interval: Calculate the lower and upper percentiles of the bootstrap distribution to construct a confidence interval. The most common confidence intervals are the 95% and 99% confidence intervals.\n",
    "\n",
    "Bootstrap provides a way to estimate the sampling variability of a statistic, especially when it's challenging or impractical to obtain additional data through repeated sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b536da-42c1-470e-b80b-6d6cd0067322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6e0422-a4d3-41d5-9783-65f3eb930908",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ec922af-d85e-45b7-a2ad-9bead47717c3",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, you would follow these steps:\n",
    "\n",
    "Data Collection: You have a sample of 50 tree heights with a sample mean of 15 meters and a standard deviation of 2 meters.\n",
    "\n",
    "Bootstrap Resampling: Create multiple bootstrap samples by randomly selecting 50 heights (with replacement) from your original sample. Repeat this process a large number of times, e.g., 10,000 times, to generate a distribution of sample means.\n",
    "\n",
    "Sample Mean Calculation: For each bootstrap sample, calculate the sample mean of the tree heights.\n",
    "\n",
    "Bootstrap Distribution: Collect all the sample means obtained from the bootstrap samples to create a distribution of sample means.\n",
    "\n",
    "Confidence Interval Calculation: Calculate the 95% confidence interval by finding the 2.5th percentile (lower bound) and the 97.5th percentile (upper bound) of the distribution of sample means. These values will provide the confidence interval for the population mean height.\n",
    "\n",
    "In Python, you can use libraries like NumPy to perform the bootstrap resampling and calculate the confidence interval. Here's a simplified example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "210c504e-67f6-4af8-9fb2-5a11a7142c59",
   "metadata": {},
   "source": [
    "by this genereated sample\n",
    "import numpy as np\n",
    "\n",
    "# Set the desired mean and standard deviation\n",
    "desired_mean = 15\n",
    "desired_std_deviation = 2\n",
    "\n",
    "# Define the sample size\n",
    "sample_size = 50\n",
    "\n",
    "# Generate random data with a standard normal distribution (mean=0, std_deviation=1)\n",
    "data = np.random.randn(sample_size)\n",
    "\n",
    "# Scale and shift the data to match the desired mean and standard deviation\n",
    "scaled_data = desired_std_deviation * data + desired_mean\n",
    "print(scaled_data)\n",
    "# Check the achieved mean and standard deviation\n",
    "achieved_mean = np.mean(scaled_data)\n",
    "achieved_std_deviation = np.std(scaled_data)\n",
    "\n",
    "print(\"Achieved Mean:\", achieved_mean)\n",
    "print(\"Achieved Standard Deviation:\", achieved_std_deviation)\n",
    "\n",
    "\n",
    "o/p\n",
    "[18.69791219 17.25313006 14.46222262 12.78694818 20.14671961 15.11843687\n",
    " 15.02785858 14.95174983 15.39616952 14.71127918 13.85267599 13.90628212\n",
    " 14.93449346 13.91315046 13.57430843 15.21286046 14.49004557 18.00798598\n",
    "  9.69806038 17.1830137  17.49217038 10.85321954 14.31462481 14.25711827\n",
    " 12.18497661 13.44436662 12.77884831 18.50454089 16.87135679 17.54311019\n",
    " 16.44334413 12.74189646 13.95095947 15.97874912 12.55574438 16.42599686\n",
    " 14.5193492  14.25035838 16.42191994 15.88852662 14.27806767 17.31865961\n",
    " 12.83787334 16.23187121 16.18620252 14.38090712 15.65226604 12.49777285\n",
    " 16.84805404 14.63019573]\n",
    "Achieved Mean: 15.032167005493942\n",
    "Achieved Standard Deviation: 2.0274193738844217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f588a-b33c-4ebb-9ed2-3d569eb70046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "282ec554-2a5e-428d-9058-65caeced91dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of sample data  15.0321670058\n",
      "\n",
      "Sizeof sample data  50\n",
      "\n",
      "Standard deviation of sample data  2.048002839832277\n",
      "\n",
      "Bootstrap 95% Confidence Interval for Mean Height (in meters): [14.478624  15.6015224]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sample_data = [18.69791219, 17.25313006, 14.46222262 ,12.78694818, 20.14671961 ,15.11843687,\n",
    " 15.02785858 ,14.95174983, 15.39616952, 14.71127918, 13.85267599 ,13.90628212,\n",
    " 14.93449346, 13.91315046, 13.57430843, 15.21286046, 14.49004557, 18.00798598,\n",
    "  9.69806038, 17.1830137,  17.49217038, 10.85321954, 14.31462481, 14.25711827,\n",
    " 12.18497661, 13.44436662, 12.77884831, 18.50454089, 16.87135679, 17.54311019,\n",
    " 16.44334413, 12.74189646, 13.95095947, 15.97874912, 12.55574438, 16.42599686,\n",
    " 14.5193492 , 14.25035838, 16.42191994, 15.88852662, 14.27806767, 17.31865961,\n",
    " 12.83787334, 16.23187121, 16.18620252, 14.38090712, 15.65226604, 12.49777285,\n",
    " 16.84805404, 14.63019573]    \n",
    "\n",
    "print('Mean of sample data ',np.mean(sample_data))\n",
    "print('\\nSizeof sample data ',np.size(sample_data))\n",
    "print('\\nStandard deviation of sample data ',np.std(sample_data, ddof=1))\n",
    "\n",
    "# No. bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "#  array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_samples)\n",
    "\n",
    "# bootstrap resampling\n",
    "for i in range(num_samples):\n",
    "    # Generate a bootstrap sample by sampling with replacement from the original sample data\n",
    "    bootstrap_sample = np.random.choice(sample_data, size=sample_size, replace=True)\n",
    "    \n",
    "    #mean of the bootstrap sample\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "#  95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"\\nBootstrap 95% Confidence Interval for Mean Height (in meters):\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5af92ed6-8bbf-48bd-bc8d-20c6e1d2bde3",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "sample_mean = 15  # Sample mean height\n",
    "sample_stddev = 2  # Sample standard deviation\n",
    "sample_size = 50  # Sample size\n",
    "\n",
    "# Generate a random sample of tree heights assuming a normal distribution\n",
    "# sample_data = np.random.normal(loc=sample_mean, scale=sample_stddev, size=sample_size)\n",
    " \n",
    "sample_data = [18.69791219, 17.25313006, 14.46222262 ,12.78694818, 20.14671961 ,15.11843687,\n",
    " 15.02785858 ,14.95174983, 15.39616952, 14.71127918, 13.85267599 ,13.90628212,\n",
    " 14.93449346, 13.91315046, 13.57430843, 15.21286046, 14.49004557, 18.00798598,\n",
    "  9.69806038, 17.1830137,  17.49217038, 10.85321954, 14.31462481, 14.25711827,\n",
    " 12.18497661, 13.44436662, 12.77884831, 18.50454089, 16.87135679, 17.54311019,\n",
    " 16.44334413, 12.74189646, 13.95095947, 15.97874912, 12.55574438, 16.42599686,\n",
    " 14.5193492 , 14.25035838, 16.42191994, 15.88852662, 14.27806767, 17.31865961,\n",
    " 12.83787334, 16.23187121, 16.18620252, 14.38090712, 15.65226604, 12.49777285,\n",
    " 16.84805404, 14.63019573]    \n",
    "# print(np.mean(sample_data))\n",
    "print(np.size(sample_data))\n",
    "\n",
    "# print(np.std(sample_data, ddof=1))\n",
    "# df1 = pd.DataFrame(sample_data)\n",
    "\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_samples)\n",
    "df = pd.DataFrame(bootstrap_means)\n",
    "\n",
    "print(bootstrap_means)\n",
    "# Perform bootstrap resampling\n",
    "for i in range(num_samples):\n",
    "    # Generate a bootstrap sample by sampling with replacement from the original sample data\n",
    "    bootstrap_sample = np.random.choice(sample_data, size=sample_size, replace=True)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height (in meters):\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
