{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431ee3eb-9a79-4f0a-bba4-4adb2b1c3b22",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93884e-4576-4cb7-985a-793fccacb75f",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to transform features so that they are within a specific range, often between 0 and 1. It's especially useful when features have different scales and you want to bring them to a common scale. The formula for Min-Max scaling is:\n",
    "\n",
    "Scaled Value=(Original Value−Min Value)/(Max Value−Min Value)\n",
    "\n",
    " \n",
    "Example: Let's say we have a dataset of house prices with values ranging from 100,000 to 1,000,000. we want to scale these values between 0 and 1. If a house price is 500,000, after Min-Max scaling, it would become:\n",
    "\n",
    "Scaled Value =(500000−100000)/(1000000−100000) =0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b3875-043e-4cc0-9309-424583567541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca3ce21-08f0-46bc-9d44-94754add96c3",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c7739-5a64-4576-b5e8-7700a62418f6",
   "metadata": {},
   "source": [
    "The Unit Vector technique (also known as Normalization) scales features to have a unit vector length, essentially bringing all feature vectors to the same scale regardless of their original ranges. It calculates the Euclidean norm of the vector and then divides each component by the norm.\n",
    "\n",
    "Example: If you have a feature vector [3, 4] and you want to normalize it, first calculate the Euclidean norm \n",
    "√(3^2+4^2) = 5\n",
    " \n",
    "Then divide each component by the norm to get [0.6, 0.8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913d202-d63b-4149-8214-979dcaf35702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a567c56-b7ff-4b2a-b052-9ed74a442cbb",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb08835-7570-4777-b8c4-e6465908d69c",
   "metadata": {},
   "source": [
    "**PCA (Principal Component Analysis):**\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original variance as possible. It does this by identifying the principal components, which are orthogonal directions in the data space along which the data varies the most.\n",
    "\n",
    "**How PCA is Used in Dimensionality Reduction:**\n",
    "PCA works by finding a new coordinate system in which the data variance is maximized along the principal components. It does this by:\n",
    "1. **Calculating the Covariance Matrix:** Compute the covariance matrix of the original data features.\n",
    "2. **Eigenvalue Decomposition:** Perform eigenvalue decomposition on the covariance matrix to find the eigenvectors and eigenvalues.\n",
    "3. **Sorting Eigenvectors:** Sort the eigenvectors in decreasing order of their corresponding eigenvalues. These eigenvectors are the principal components.\n",
    "4. **Projection:** Project the original data onto the new coordinate system defined by the principal components.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have data on the height, weight, and age of individuals. Each person is represented by a vector [height, weight, age]. we want to reduce the dimensionality while preserving the most important information.\n",
    "\n",
    "1. **Data Collection:** Collect data from a group of individuals, resulting in a dataset with three features: height, weight, and age.\n",
    "\n",
    "2. **Covariance Matrix:** Calculate the covariance matrix of the three features to understand their relationships and how they vary together.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** Perform eigenvalue decomposition on the covariance matrix to find the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Sorting Eigenvectors:** Sort the eigenvectors based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue is the first principal component, the second-highest eigenvalue corresponds to the second principal component, and so on.\n",
    "\n",
    "5. **Projection:** Project the original data onto the new coordinate system defined by the principal components. This will result in a transformed dataset with reduced dimensions. You can choose to retain only the top-k principal components, reducing the dimensionality from three to k.\n",
    "\n",
    "By selecting a lower number of principal components (k) while still capturing a significant amount of variance, thus we achieve dimensionality reduction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac2a0000-273a-401c-979e-41755ca3dec1",
   "metadata": {},
   "source": [
    "By selecting a lower number of principal components (k) while still capturing a significant amount of variance,thus we achieve dimensionality reduction. In this example, you might find that much of the variability in the data can be explained by just one or two principal components, such as body size.\n",
    "\n",
    "**Benefits of PCA in Dimensionality Reduction:**\n",
    "- **Reduced Complexity:** It simplifies data representation, making it easier to visualize and analyze.\n",
    "- **Noise Reduction:** It reduces the impact of noisy or irrelevant features, leading to a more robust model.\n",
    "- **Improved Efficiency:** With fewer features, computations are faster, making algorithms more efficient.\n",
    "- **Improved Generalization:** Reduced dimensionality can lead to better generalization and less overfitting.\n",
    "\n",
    "Keep in mind that PCA is an unsupervised technique that focuses on data variance, and the interpretability of the new dimensions might be challenging in certain contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d644679-1c32-4b2d-abda-a6d9f9f9e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3f816d-71a2-40bf-a75a-ad5e43e22520",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de989d6-f01a-4e03-8970-95ea97d0d1ef",
   "metadata": {},
   "source": [
    "**Relationship Between PCA and Feature Extraction:**\n",
    "PCA is a technique that can be used for feature extraction. Feature extraction involves transforming the original features into a new set of features while retaining the most important information. PCA achieves this by creating new features, called principal components, which are linear combinations of the original features. These principal components capture the directions of maximum variance in the data.\n",
    "\n",
    "**Using PCA for Feature Extraction:**\n",
    "PCA can be applied as a feature extraction technique when you want to reduce the dimensionality of your data while preserving as much relevant information as possible. Here's how we can use PCA for feature extraction:\n",
    "\n",
    "1. **Data Preparation:** Collect and preprocess your data, ensuring it's well-structured and ready for analysis.\n",
    "\n",
    "2. **Standardization:** Standardize the data by subtracting the mean and dividing by the standard deviation. This step is crucial for PCA since it's sensitive to the scale of the features.\n",
    "\n",
    "3. **PCA Computation:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "   - Perform eigenvalue decomposition on the covariance matrix to find the eigenvectors and eigenvalues.\n",
    "   - Sort the eigenvectors in decreasing order of their eigenvalues to get the principal components.\n",
    "\n",
    "4. **Selecting Principal Components:** Choose the top-k principal components that capture a significant portion of the data's variance. You can decide the number of components to retain based on a desired percentage of variance explained (e.g., 95%).\n",
    "\n",
    "5. **Projecting Data:** Project the original standardized data onto the selected principal components. These projected values become the extracted features, which are the transformed representations of the original data.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset of images with pixel values as features. Each image is represented as a vector of pixel intensities. we want to extract essential features from these images for further analysis.\n",
    "\n",
    "1. **Data Collection:** Collect a dataset of images, where each image is represented by a vector of pixel values.\n",
    "\n",
    "2. **Standardization:** Standardize the pixel values across all images by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "3. **PCA Computation:** Calculate the covariance matrix of the standardized pixel values and perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Selecting Principal Components:** Choose a certain number of principal components or set a threshold for the percentage of variance explained. For example, we might decide to retain the top 50 principal components, which account for 90% of the variance.\n",
    "\n",
    "5. **Projecting Data:** Project the standardized pixel values of each image onto the selected principal components. These projected values become the new features, effectively extracting essential information from the images.\n",
    "\n",
    "In this example, PCA extracts features that represent the main patterns or structures in the images. These features can be used for various tasks like image classification, clustering, or visualization.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c3edae4-997c-46bf-a23c-d2cce1abd936",
   "metadata": {},
   "source": [
    "\n",
    "**Benefits of PCA for Feature Extraction:**\n",
    "- Reduces dimensionality while preserving relevant information.\n",
    "- Removes noise and redundant features.\n",
    "- Enhances the efficiency of subsequent algorithms.\n",
    "- Helps in dealing with multicollinearity among features.\n",
    "\n",
    "However, it's important to note that PCA is an unsupervised technique and might not always result in interpretable features, especially in complex datasets like images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2af51-fa5a-4a58-a414-683cad83b235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47e9cf2f-5c50-4787-81ef-6cb57816d53e",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf89fb08-2ec8-418d-9acf-c76ad56549d5",
   "metadata": {},
   "source": [
    "**Min-Max Scaling:**\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale the features of a dataset to a specific range, typically between 0 and 1. This scaling method ensures that all features have the same scale, making them comparable and suitable for certain machine learning algorithms and analyses.\n",
    "\n",
    "**Using Min-Max Scaling for Preprocessing:**\n",
    "We can use Min-Max scaling to preprocess the features in your food delivery recommendation system dataset:\n",
    "\n",
    "1. **Data Collection:** Collect the dataset containing features such as price, rating, and delivery time for the food items.\n",
    "\n",
    "2. **Understanding the Features:** Understand the ranges and distributions of the features. For instance, prices might range from a few Rs to a few hundred Rs, ratings might range from 1 to 5, and delivery times might be in minutes.\n",
    "\n",
    "3. **Min-Max Scaling:**\n",
    "   - For each feature, calculate the minimum and maximum values in the dataset.\n",
    "   - Apply the Min-Max scaling formula to scale the values of each feature to a common range (usually 0 to 1):\n",
    "   \n",
    "     x_scaled = (x - x_min)/(x_max - x_min) \n",
    "     \n",
    "     Where:\n",
    "     - x is the original value of the feature.\n",
    "     - x_min is the minimum value of the feature in the dataset.\n",
    "     - x_max is the maximum value of the feature in the dataset.\n",
    "     - x_scaled is the scaled value of the feature.\n",
    "\n",
    "4. **Implementation:** Use the scaled values as the new values of your features. Replace the original values with their scaled counterparts.\n",
    "\n",
    "**Example:**\n",
    "Suppose we have a dataset for food items with the following features:\n",
    "- Price (ranging from 40 Rs to 400 Rs)\n",
    "- Rating (ranging from 1 to 5)\n",
    "- Delivery Time (ranging from 15 to 60 minutes)\n",
    "\n",
    "We apply Min-Max scaling to these features.\n",
    "\n",
    "1. **Data Collection:** Collect the dataset with food item features.\n",
    "\n",
    "2. **Understanding the Features:** We know the ranges of each feature.\n",
    "\n",
    "3. **Min-Max Scaling:**\n",
    "   - Calculate the minimum and maximum values for each feature.\n",
    "   - Apply the Min-Max scaling formula to each feature value to scale it to the range [0, 1].\n",
    "\n",
    "4. **Implementation:** Replace the original feature values with their scaled values.\n",
    "\n",
    "After Min-Max scaling, all features will have values between 0 and 1, making them comparable and suitable for algorithms that are sensitive to feature scales, such as gradient descent-based optimization algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ebcbd18-5165-43ae-a350-952cf501b73d",
   "metadata": {},
   "source": [
    "**Benefits of Min-Max Scaling:**\n",
    "- Ensures that all features have the same scale, preventing one feature from dominating the others.\n",
    "- Makes features suitable for algorithms that rely on distance measures or gradient descent.\n",
    "- Can improve the performance and convergence of certain machine learning algorithms.\n",
    "\n",
    "However, Min-Max scaling might not be suitable for all types of data, especially if the data has outliers that could be distorted by the scaling process. In such cases, other scaling techniques like Standardization might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfac37-481e-4a5a-873f-10ea5eae11b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a8e1a9b-bb9d-47bd-9302-8dfcf6918fb8",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcd74e-f6c1-46bb-981e-33a7e2bf5f04",
   "metadata": {},
   "source": [
    "**Using PCA to Reduce Dimensionality for Stock Price Prediction:**\n",
    "\n",
    "When dealing with a large number of features in a stock price prediction project, PCA can be employed to reduce the dimensionality of the dataset while retaining the most important information. Here's how we can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "Suppose we have a dataset with 20 features, including financial metrics like earnings per share, price-to-earnings ratio, debt-to-equity ratio, and market trends like trading volume and volatility. we want to use PCA to reduce the dimensionality of this dataset.\n",
    "\n",
    "1. **Data Collection:** Collect the dataset with stock-related features.\n",
    "\n",
    "2. **Feature Engineering:** Create relevant features capturing financial and market indicators.\n",
    "\n",
    "3. **Standardization:** Standardize the features across all data points.\n",
    "\n",
    "4. **PCA Computation:** Calculate the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "5. **Eigenvalue Decomposition:** Perform eigenvalue decomposition on the covariance matrix to get eigenvectors and eigenvalues.\n",
    "\n",
    "6. **Sorting Eigenvectors:** Sort eigenvectors by decreasing eigenvalues.\n",
    "\n",
    "7. **Choosing Principal Components:** Decide to retain the top-k principal components, capturing a desired amount of variance (e.g., 95%).\n",
    "\n",
    "8. **Projecting Data:** Project the standardized feature matrix onto the selected principal components.\n",
    "\n",
    "9. **Modeling and Evaluation:** Train the stock price prediction model using the reduced-dimensional data and evaluate its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90b029aa-9eaa-4a92-b9f4-2a5cea3aee04",
   "metadata": {},
   "source": [
    "1. **Data Preparation:**\n",
    "   - Collect and preprocess the dataset containing various features such as company financial data and market trends.\n",
    "\n",
    "2. **Feature Engineering:**\n",
    "   - Create relevant features that capture the financial metrics, market indicators, and other attributes important for predicting stock prices.\n",
    "\n",
    "3. **Standardization:**\n",
    "   - Standardize the features to ensure that they have a mean of 0 and a standard deviation of 1. This step is crucial for PCA since it's sensitive to the scale of the features.\n",
    "\n",
    "4. **PCA Computation:**\n",
    "   - Calculate the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "5. **Eigenvalue Decomposition:**\n",
    "   - Perform eigenvalue decomposition on the covariance matrix to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "6. **Sorting Eigenvectors:**\n",
    "   - Sort the eigenvectors in decreasing order of their eigenvalues. These eigenvectors are the principal components.\n",
    "\n",
    "7. **Choosing Principal Components:**\n",
    "   - Decide how many principal components to retain. You can choose based on a desired percentage of variance explained (e.g., 95%).\n",
    "\n",
    "8. **Projecting Data:**\n",
    "   - Project the standardized feature matrix onto the selected principal components. These projected values become the new features, effectively reducing the dimensionality of the dataset.\n",
    "\n",
    "9. **Modeling and Evaluation:**\n",
    "   - Train your stock price prediction model using the reduced-dimensional dataset.\n",
    "   - Evaluate the model's performance on validation or test data using appropriate evaluation metrics.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Benefits of Using PCA for Dimensionality Reduction:**\n",
    "- Reduces computation time and memory requirements for training models.\n",
    "- Addresses the curse of dimensionality, which can lead to overfitting and poor generalization.\n",
    "- Enhances model interpretability by focusing on the most important information.\n",
    "\n",
    "However, keep in mind that while PCA reduces dimensionality, it might make the transformed features less interpretable. Also, PCA assumes that the variance in the data is related to important information, which might not always be the case in financial data. Experimentation and domain knowledge are essential to make informed decisions about feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c201443-7bb5-43a2-bde5-22dc4645e9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0012eb6-eb75-46af-8b58-f3cafbda1de0",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform thevalues to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55f70d-8050-4d1b-bcd2-10454ea314e9",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you can use the following formula:\n",
    "\n",
    " - x_scaled = (x - x_min)/(x_max - x_min)\n",
    "\n",
    " **Where:**\n",
    "- x is the original value of the feature.\n",
    "- x_min is the minimum value of the feature in the dataset.\n",
    "- x_max is the maximum value of the feature in the dataset.\n",
    "- x_scaled is the scaled value of the feature.\n",
    "\n",
    "\n",
    "Given the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "1. Calculate x_min and x_max:\n",
    "    - x_min =1\n",
    "    - x_max = 20\n",
    "\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset:\n",
    "   - for x = 1:\n",
    "     x_scaled = 2(1-1)/(20-1) -1 = -1\n",
    "   - for x = 5:  \n",
    "     x_scaled = 2(5-1)/(20-1) -1 = -0.6\n",
    "   - for x = 10:  \n",
    "     x_scaled = 2(10-1)/(20-1) -1 = -0.2\n",
    "   - for x = 15:  \n",
    "     x_scaled = 2(15-1)/(20-1) -1 = 0.2\n",
    "   - for x = 20:  \n",
    "     x_scaled = 2(20-1)/(20-1) -1 = 1 \n",
    "  \n",
    "  \n",
    "  \n",
    "The scaled dataset with values transformed to the range of -1 to 1 will be: [-1, -0.6, -0.2, 0.2, 1]\n",
    "\n",
    "After Min-Max scaling, all the values will fall within the desired range of -1 to 1, making them comparable and suitable for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcdc52-ba4f-4c37-917b-a97f87e24496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71101d6-e747-40d7-aae2-d2e13a610279",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49939d-acc8-4f87-a7cb-705fd56b7d34",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], we can follow these steps:\n",
    "\n",
    "1. **Data Preparation:** Collect and preprocess the dataset, ensuring that it's well-structured and ready for analysis.\n",
    "\n",
    "2. **Standardization:** Standardize the features (height, weight, age, and blood pressure) to have zero mean and unit variance. This is important for PCA, which is sensitive to feature scales.\n",
    "\n",
    "3. **PCA Computation:**\n",
    "   - Calculate the covariance matrix of the standardized feature matrix.\n",
    "   - Perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Choosing Principal Components:**\n",
    "   - Sort the eigenvectors based on their corresponding eigenvalues in decreasing order.\n",
    "   - Decide on the number of principal components to retain.\n",
    "\n",
    "5. **Projection:**\n",
    "   - Project the standardized feature matrix onto the selected principal components.\n",
    "\n",
    "The number of principal components to retain depends on the desired amount of variance explained. we can set a threshold for the cumulative explained variance or decide based on the elbow point in the scree plot (a plot of eigenvalues). A common approach is to choose enough principal components to retain a significant percentage of the total variance, such as 95% or 99%.\n",
    "\n",
    "In case, the decision of how many principal components to retain would depend on the dataset and our goals. Let's consider a scenario where we want to retain 95% of the variance. Here's how we might make that decision:\n",
    "\n",
    "1. Calculate the total variance of the original dataset based on the eigenvalues.\n",
    "2. Calculate the cumulative explained variance by summing the eigenvalues from largest to smallest.\n",
    "3. Determine the number of principal components needed to achieve 95% of the total variance.\n",
    "\n",
    "For example, if the first two principal components explain 90% of the total variance, we might decide to retain those two components. This choice strikes a balance between reducing dimensionality and retaining a significant amount of information.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41b7229f-89d6-474c-b8ba-84b9878c81b7",
   "metadata": {},
   "source": [
    "Keep in mind that while PCA can reduce dimensionality, it might not always lead to interpretable features, especially when dealing with categorical features like gender. Additionally, PCA assumes that high-variance dimensions are more important, which might not always hold true for all datasets. Experimentation and domain knowledge are essential for making informed decisions about the number of principal components to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
