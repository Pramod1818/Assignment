{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d193c47-a886-4141-a32b-f141a57952ac",
   "metadata": {},
   "source": [
    "Q1. Write a Python code to implement the KNN classifier algorithm on load_iris dataset in\n",
    "sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7f440e-4456-4a03-b57c-88e64c03dba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#  KNN classifier with k=3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier \n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aab52a-21c1-4e37-94b6-030aa2b7085d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d31ce76-37b9-4ec0-b3d5-7f4c5285f0a7",
   "metadata": {},
   "source": [
    "Q2. Write a Python code to implement the KNN regressor algorithm on load_boston dataset in\n",
    "sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c2e5645-72b3-4125-9162-b0498a797226",
   "metadata": {},
   "source": [
    "The code snippet you provided processes the data loaded from the URL and separates it into two parts: `data` and `target`. Let me break down what each part does:\n",
    "\n",
    "1. `data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])`:\n",
    "\n",
    "   - `raw_df.values` converts the DataFrame `raw_df` into a NumPy array.\n",
    "   - `[::2, :]` selects every second row of the array starting from the first row (i.e., even rows).\n",
    "   - `[1::2, :2]` selects every second row starting from the second row (i.e., odd rows) and only the first two columns.\n",
    "\n",
    "   The `np.hstack` function horizontally stacks (concatenates) these two selected parts, effectively interleaving the even and odd rows and keeping only the first two columns from the odd rows. This is used to create the `data` variable, which likely represents the input features for a machine learning model.\n",
    "\n",
    "2. `target = raw_df.values[1::2, 2]`:\n",
    "\n",
    "   - `raw_df.values` again converts the DataFrame `raw_df` into a NumPy array.\n",
    "   - `[1::2, 2]` selects every second row starting from the second row (i.e., odd rows) and extracts only the third column.\n",
    "\n",
    "   This line of code creates the `target` variable, which likely represents the target variable or labels for a machine learning model.\n",
    "\n",
    "In summary, the code is preparing the data and target variables for a machine learning task, assuming that the input features are contained in the even rows and the target values are in the third column of the odd rows of the loaded data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "878d9f5e-f976-4d9a-9c98-91f46a086041",
   "metadata": {},
   "source": [
    "# from sklearn.datasets import load_boston  is not working\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, delim_whitespace=True, skiprows=22, header=None)\n",
    "\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fa24e7a-2890-4e64-bde1-dc0e1b061c37",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.preprocessing._encoders\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.datasets._openml\")\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Boston Housing dataset\n",
    "boston = fetch_openml(data_id=42165, as_frame=True)\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('num', 'passthrough')])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "# Bundle preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# KNN regressor model\n",
    "knn_regressor = KNeighborsRegressor()\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', knn_regressor)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'model__n_neighbors': [1, 3, 5, 7, 9],  # can specify different values for k\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "# GridSearchCV object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#best model and its performance\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Best Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a52e08e4-850c-42a3-807c-673e50570efb",
   "metadata": {},
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = fetch_openml(data_id=42165, as_frame=True)\n",
    "\n",
    "# Access data and target variables\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Define preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('num', 'passthrough')])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "# Bundle preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Define the KNN regressor model\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', knn_regressor)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436a45e1-fdbd-4e73-a488-193b105e1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2411824950.714385\n",
      "R-squared: 0.6714757690673427\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.preprocessing._encoders\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.datasets._openml\")\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Boston Housing dataset\n",
    "boston = fetch_openml(data_id=42165, as_frame=True)\n",
    "\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# numeric and categorical columns\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                      ('num', 'passthrough')])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "#  preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# KNN regressor model\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', knn_regressor)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc8359-44ed-46e8-bb0d-9133448ce3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b31ef14c-af76-4ac2-81d1-de4502590ced",
   "metadata": {},
   "source": [
    "Q3. Write a Python code snippet to find the optimal value of K for the KNN classifier algorithm using\n",
    "cross-validation on load_iris dataset in sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97583e13-9d06-4eaf-9952-080c2f83545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal K: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# cross-validation to find the optimal K\n",
    "k_values = range(1, 31)  # Test K values from 1 to 30\n",
    "cv_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_classifier.n_neighbors = k\n",
    "    scores = cross_val_score(knn_classifier, X, y, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# optimal K with the highest cross-validation score\n",
    "optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "print(\"Optimal K:\", optimal_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7009c7f-3e4c-409b-8c5c-65fc7b0d26ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f545d4-a077-43ad-9044-8a07966c4125",
   "metadata": {},
   "source": [
    "Q4. Implement the KNN regressor algorithm with feature scaling on load_boston dataset in\n",
    "sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2551a1dc-79ab-424c-90c3-d239fec20633",
   "metadata": {},
   "source": [
    "# Convert the data and target variables to a DataFrame\n",
    "data_df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "target_df = pd.Series(data=boston.target, name=\"target\")\n",
    "\n",
    "# Combine data and target into a single DataFrame if needed\n",
    "# boston_df = pd.concat([data_df, target_df], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f97c2c8f-0759-4392-a18b-24f907f0820c",
   "metadata": {},
   "source": [
    "type(target)\n",
    "o/p - pandas.core.series.Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0227a107-8d74-46b1-b868-769b6899b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error with Feature Scaling and Encoding: 1528025980.7851343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "boston = fetch_openml(data_id=42165, as_frame=True)\n",
    "\n",
    "numeric_cols = boston.data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = boston.data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error with Feature Scaling and Encoding:\", mse)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5987515e-e8f0-4544-abb7-33c7847013ac",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = fetch_openml(data_id=42165, as_frame=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = boston.data.select_dtypes(include=['number']).columns\n",
    "categorical_cols = boston.data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Split the data\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Bundle preprocessing for numeric and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Define the KNN regressor model\n",
    "model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)])\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance (e.g., mean squared error)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error with Feature Scaling and Encoding:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ca65e-5f0f-440e-aa91-b7a1472f8d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b3918a-eb80-4119-8421-78ff4f1efdeb",
   "metadata": {},
   "source": [
    "Q5. Write a Python code snippet to implement the KNN classifier algorithm with weighted voting on\n",
    "load_iris dataset in sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1af060cb-ce31-4a24-9bb8-2ea554705fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Weighted Voting: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with Weighted Voting:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a6db4-5f4c-4f91-b04d-3b029fa1c965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95d9da44-40d7-45c1-a7e3-518b8a8f1c54",
   "metadata": {},
   "source": [
    "Q6. Implement a function to standardise the features before applying KNN classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "118ddf74-7072-40e3-9822-09dcb19bd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "def standardize_features_and_knn(X_train, X_test, y_train, y_test, n_neighbors=3):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #  KNN classifier\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Fit the classifier to the standardized training data\n",
    "    knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d863fb4-295f-40ab-a2c7-596115c38a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Iris dataset: 0.98\n"
     ]
    }
   ],
   "source": [
    "accuracy = standardize_features_and_knn(X_train, X_test, y_train, y_test, n_neighbors=3)\n",
    "print(\"Accuracy on Iris dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82ddb0-58b2-44aa-913c-c79aa350305d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74bc10c5-da7f-44a4-80ee-98d85069f9c7",
   "metadata": {},
   "source": [
    "Q7. Write a Python function to calculate the euclidean distance between two points.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a5a94ed-29f1-40e2-86bd-d73fe2b64f6a",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance1(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "print(euclidean_distance([1,2],[3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70dc7b4c-87d0-4049-9b73-a779521f4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 3.605551275463989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "  \n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "\n",
    "    if point1.shape != point2.shape:\n",
    "        raise ValueError(\"Both points must have the same number of dimensions.\")\n",
    "\n",
    "    distance = np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "    return distance\n",
    "\n",
    "point_a = [1, 7, 2]\n",
    "point_b = [3, 4, 2]\n",
    "\n",
    "distance = euclidean_distance(point_a, point_b)\n",
    "print(\"Euclidean Distance:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058a2bc-e910-4ba4-b4c3-c68534bd8ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "806d12d4-1d61-4e69-a851-67b663be11cf",
   "metadata": {},
   "source": [
    "Q8. Write a Python function to calculate the manhattan distance between two points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e35bb98-01dd-4f7e-a821-a9e3e86b9a48",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def manhattan_distance(point1, point2):\n",
    "    return np.sum(np.abs(point1 - point2))\n",
    "\n",
    "dist = manhattan_distance(-10,40)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd6fc504-7053-48e1-b2af-467d2f80b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance: 12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def manhattan_distance(point1, point2):\n",
    "    \n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "\n",
    "    if point1.shape != point2.shape:\n",
    "        raise ValueError(\"Both points must have the same number of dimensions.\")\n",
    "\n",
    "    distance = np.sum(np.abs(point1 - point2))\n",
    "    return distance\n",
    "\n",
    "point_a = [1, 2, 3]\n",
    "point_b = [4, 5, 9]\n",
    "\n",
    "distance = manhattan_distance(point_a, point_b)\n",
    "print(\"Manhattan Distance:\", distance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
