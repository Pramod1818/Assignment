{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d02085f-9db3-49b0-92d7-b2d37ded67e5",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724138ea-17f4-4c27-b4c0-45de408a09b9",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and versatile supervised machine learning algorithm used for both classification and regression tasks. In KNN, the prediction for a new data point is based on the majority class (for classification) or the average value (for regression) among its K nearest neighbors in the training dataset. The \"nearest\" neighbors are determined using a distance metric, typically Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e47bf2-6b0d-434e-a60d-96cfd9b7e982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a2c09e-4274-4483-a808-fb03e83fc700",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d8e3d-3767-4c74-9d0e-91c24b9fa45a",
   "metadata": {},
   "source": [
    "Choosing the value of K in KNN is a crucial decision that can impact the model's performance. The selection of K depends on factors like the nature of the data, the problem at hand, and the desired trade-off between bias and variance. A few common methods for choosing K include:\n",
    "\n",
    "Trying various values of K and using cross-validation to select the one that minimizes prediction errors.\n",
    "Using domain knowledge or exploring the data to determine a reasonable range for K.\n",
    "Considering the square root of the number of data points as a starting point, which often works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa032371-aa60-427d-aa4d-3f125c9a8540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24f2b94-b0f5-42d6-83ef-968c7591579a",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8d4fc-40f2-4db5-bd33-fa04a9c17489",
   "metadata": {},
   "source": [
    "The primary difference between KNN classifier and KNN regressor lies in their tasks:\n",
    "\n",
    "- KNN Classifier: KNN classifier is used for classification tasks, where the goal is to predict the class or category of a data point. It assigns a class label to a new data point based on the majority class among its K nearest neighbors.\n",
    "\n",
    "- KNN Regressor: KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value for a data point. It calculates the average value of the target variable among its K nearest neighbors to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbc926-0fe7-496e-ae22-b8e3603ca2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508dc4f8-f23b-4bdf-ab4d-72ab9303ab2e",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfffa77-9b58-41c0-8309-d8f935657049",
   "metadata": {},
   "source": [
    "The performance of a KNN model is typically evaluated using appropriate metrics based on the task:\n",
    "\n",
    "For Classification:\n",
    "\n",
    "- Accuracy: The proportion of correctly classified data points.\n",
    "- Precision, Recall, F1-Score: Measures for evaluating class-specific performance, especially useful for imbalanced datasets.\n",
    "- ROC Curve and AUC: For binary classification problems, particularly when considering trade-offs between true positive and false positive rates.\n",
    "\n",
    "For Regression:\n",
    "\n",
    "- Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.\n",
    "- R-squared (R²): Measures the proportion of the variance in the target variable explained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465c819-9441-40b1-9ee5-685f2c96d752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c8914fd-d797-4447-aabf-734399735dd5",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede5f5b-00ce-454d-9516-b3e49ab2b627",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" in KNN refers to the phenomenon where the performance of KNN deteriorates as the dimensionality (number of features) of the dataset increases. It occurs because, in high-dimensional spaces, data points become more spread out, making it challenging to find \"nearest neighbors\" effectively. As a result, KNN may require a large K to compensate for the sparsity of data points, which can lead to increased computational complexity and less discriminative power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61671c8-a561-4c43-9954-d2421cbaf445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae11d648-5b06-4a86-8592-e1a29425a4ca",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafad124-fbcf-4e12-81b6-acd5d9653736",
   "metadata": {},
   "source": [
    "To Handling missing values in KNN some Common approaches are:\n",
    "\n",
    "- Imputation: Fill missing values with estimated values (e.g., mean, median, mode) based on the values of the nearest neighbors.\n",
    "- Ignore missing values: Exclude data points with missing values during the KNN search, which may reduce the dataset size but retains available information.\n",
    "- Advanced imputation methods: Use machine learning models to predict missing values based on other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf95e0-66df-4260-bdd5-1377dd4c20c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58f4b31c-5eee-4ba1-ab06-a78cc4715120",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c0b977-5292-496c-8645-5043cc4e7385",
   "metadata": {},
   "source": [
    "KNN Classifier: Suitable for problems where the target variable is categorical or involves class labels. It's used for tasks like image classification, text categorization, and spam detection.\n",
    "\n",
    "KNN Regressor: Appropriate for problems where the target variable is continuous or numerical. It's used for tasks like predicting house prices, stock prices, and temperature forecasting.\n",
    "\n",
    "The choice between KNN classifier and regressor depends on the nature of the problem and the type of output you want (categorical or numerical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd910b26-89b5-4a0c-82a5-837771003d0f",
   "metadata": {},
   "source": [
    "Comparing and contrasting the performance of the KNN classifier and KNN regressor helps us understand when to use each type based on the nature of the problem:\n",
    "\n",
    "**KNN Classifier:**\n",
    "\n",
    "1. **Problem Type:** Suitable for classification tasks where the goal is to predict the class or category of a data point.\n",
    "2. **Output:** Provides class labels as predictions.\n",
    "3. **Use Cases:** Commonly used for problems like spam email detection, sentiment analysis, image classification, and medical diagnosis (e.g., disease classification).\n",
    "4. **Performance Metrics:** Evaluated using metrics such as accuracy, precision, recall, F1-score, ROC curve, and AUC.\n",
    "5. **Decision Boundaries:** KNN classifier decision boundaries are typically non-linear and can adapt to complex class distributions.\n",
    "\n",
    "**KNN Regressor:**\n",
    "\n",
    "1. **Problem Type:** Suitable for regression tasks where the goal is to predict a continuous numerical value for a data point.\n",
    "2. **Output:** Provides numerical predictions as outputs.\n",
    "3. **Use Cases:** Used for tasks like house price prediction, stock price forecasting, demand forecasting, and temperature prediction.\n",
    "4. **Performance Metrics:** Evaluated using metrics such as Mean Squared Error (MSE) and R-squared (R²).\n",
    "5. **Prediction:** KNN regressor predicts continuous values by averaging the target values of its nearest neighbors.\n",
    "\n",
    "**Comparing Performance:**\n",
    "\n",
    "- **KNN Classifier:** Works well when the target variable has distinct categories or classes. It can handle multi-class classification problems. It's sensitive to the choice of K, and the optimal K depends on the data and problem.\n",
    "\n",
    "- **KNN Regressor:** Appropriate when the target variable is continuous and predictions need to be in the form of numerical values. Like the classifier, it's sensitive to the choice of K, and tuning K is essential.\n",
    "\n",
    "**Choosing Between KNN Classifier and KNN Regressor:**\n",
    "\n",
    "1. **Nature of the Target Variable:** Choose based on whether the target variable is categorical (use KNN classifier) or continuous (use KNN regressor).\n",
    "\n",
    "2. **Task Requirements:** Consider the specific requirements of the problem. For example, if we need to predict house prices, KNN regressor is suitable, whereas for sentiment analysis, KNN classifier is more appropriate.\n",
    "\n",
    "3. **Evaluation Metrics:** The choice may also depend on the evaluation metrics. If we are primarily interested in classification metrics like accuracy and F1-score, use KNN classifier. For regression tasks, MSE and R² are more relevant.\n",
    "\n",
    "4. **Data Complexity:** Consider the complexity of the data and whether the relationship between features and the target is more linear or non-linear. KNN classifier can handle complex non-linear boundaries, while KNN regressor can capture non-linear relationships in the data.\n",
    "\n",
    "5. **Domain Knowledge:** Domain knowledge and understanding of the problem can guide the choice between classification and regression. Some problems may naturally require one approach over the other.\n",
    "\n",
    "The choice between KNN classifier and KNN regressor depends on the problem type, target variable nature, evaluation metrics, data complexity, and domain knowledge. Both have their strengths and are versatile techniques for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe173725-8c99-4b62-b9af-19f76681c5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1179bfe-70e4-4225-9a69-3daf4a68d70f",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6b7d0-bf09-4913-b0ff-f3e7fbeee3a9",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "- Simple and easy to understand.\n",
    "- Non-parametric and can capture complex relationships.\n",
    "- Versatile and suitable for both classification and regression.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Sensitive to the choice of K.\n",
    "- Computationally expensive for large datasets.\n",
    "- Prone to noise and outliers.\n",
    "- Curse of dimensionality in high-dimensional spaces.\n",
    "\n",
    "To address these weaknesses:\n",
    "\n",
    "- Use cross-validation to choose an optimal K.\n",
    "- Apply dimensionality reduction techniques when dealing with high-dimensional data.\n",
    "- Handle outliers or noisy data points appropriately.\n",
    "- Consider distance weighting or use alternative distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239246b2-f0a8-49a5-8c02-ad6e90db4fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fb8ee6b-8522-471d-9c6c-d92c42782af7",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d3a58-e018-4504-9af4-459191d83578",
   "metadata": {},
   "source": [
    "- Euclidean Distance:\n",
    "Euclidean distance measures the straight-line distance (as the crow flies) between two data points in a geometric space. It is calculated as the square root of the sum of squared differences between corresponding coordinates of the points. In KNN, Euclidean distance is commonly used when dealing with continuous data.\n",
    "\n",
    "- Manhattan Distance: Manhattan distance (also known as L1 distance or taxicab distance) measures the distance between two points as the sum of the absolute differences between their coordinates. It is named after the grid-like layout of streets in Manhattan. Manhattan distance is often used when dealing with data that follows a grid or lattice structure, such as images or text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c0043-523c-4fbf-9ad1-5a364335461e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cf4db29-ad01-4c5c-b47e-8ce071f37e0b",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a8708-b620-4035-8ee5-d0246a1fcfac",
   "metadata": {},
   "source": [
    "Feature scaling is important in KNN because the algorithm relies on distance metrics to determine the similarity between data points. When features have different scales or units, those with larger scales can dominate the distance calculations. Feature scaling ensures that all features contribute equally to the distance measurements. Common methods for feature scaling in KNN include Min-Max scaling (scaling to a specific range) and Z-score normalization (scaling to have a mean of 0 and standard deviation of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf2080-e899-4529-aee2-730156d1df96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
